tag_id,cluster,bookmark_id,url,title,description,language,created_at,updated_at,key,name,topic
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,a4f37ecc-172a-48e4-8570-8d059398d77d,https://www.latent.space/p/2025-papers,The 2025 AI Engineering Reading List,"We picked 50 paper/models/blogs across 10 fields in AI Eng: LLMs, Benchmarks, Prompting, RAG, Agents, CodeGen, Vision, Voice, Diffusion, Finetuning. If you're starting from scratch, start here.",en,2025-10-23 12:35:18.812,2025-10-23 12:35:18.812,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
d73bda20-81cf-4ac2-8d73-f782b68a1901,4,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,ai,ai,"""Advances in Reasoning and Learning for AI Agents"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,a4f37ecc-172a-48e4-8570-8d059398d77d,https://www.latent.space/p/2025-papers,The 2025 AI Engineering Reading List,"We picked 50 paper/models/blogs across 10 fields in AI Eng: LLMs, Benchmarks, Prompting, RAG, Agents, CodeGen, Vision, Voice, Diffusion, Finetuning. If you're starting from scratch, start here.",en,2025-10-23 12:35:18.812,2025-10-23 12:35:18.812,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
23ad99a1-37e5-43dd-bd41-3ac8589cfe6f,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,engineering,engineering,"""Evaluating Performance Capabilities of Augmented Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,a4f37ecc-172a-48e4-8570-8d059398d77d,https://www.latent.space/p/2025-papers,The 2025 AI Engineering Reading List,"We picked 50 paper/models/blogs across 10 fields in AI Eng: LLMs, Benchmarks, Prompting, RAG, Agents, CodeGen, Vision, Voice, Diffusion, Finetuning. If you're starting from scratch, start here.",en,2025-10-23 12:35:18.812,2025-10-23 12:35:18.812,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
886e8c54-87e1-4933-85d4-8520c12321cb,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,reading,reading,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,research,research,"""Enhancing Query Response Accuracy in Language Models"""
bc9adf25-5567-4aba-abfe-3fb1f5d0e146,16,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,research,research,"""Enhancing Query Response Accuracy in Language Models"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
b0e3906d-3109-41a0-befa-8de4bd72b3a1,4,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,reasoning,reasoning,"""Advances in Reasoning and Learning for AI Agents"""
7f69db37-a871-44b8-9d21-499c181457a6,6,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
7f69db37-a871-44b8-9d21-499c181457a6,6,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,model,model,"""Advancements in Unified Architectures for Language Model Implementation"""
9f0396d7-36da-4070-8017-cbef67693406,4,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
9f0396d7-36da-4070-8017-cbef67693406,4,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,reinforcement,reinforcement,"""Advances in Reasoning and Learning for AI Agents"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,06ba62ab-870f-432a-87a6-a5be07f7b2e6,https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo,"Recent reasoning research: GRPO tweaks, base model RL, and data curation",The papers I endorse as worth reading among a cresting wave of reasoning research.,en,2025-10-23 12:35:36.480,2025-10-23 12:35:36.480,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
3589c964-5bf3-4f4c-805e-0c790a23d76a,5,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,curation,curation,"""Advancements in Context-Aware Language Model Performance and Applications"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,802b1856-b4cf-4b0b-bcac-b65149c9213a,https://openai.com/index/chatgpt/,Introducing ChatGPT,"We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",en-US,2025-10-23 12:36:13.336,2025-10-23 12:36:13.336,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
8928bb71-ccb2-4f99-9dde-a961c7d9e36c,14,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,chatgpt,chatgpt,"""Advancements in Conversational AI and Language Model Architectures"""
117fd427-5286-461b-91f5-e19f46771a26,1,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,api,api,"""Integration of Large Language Models in Code Development"""
117fd427-5286-461b-91f5-e19f46771a26,1,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,api,api,"""Integration of Large Language Models in Code Development"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,1785c3fe-0ec1-4208-ada2-f0366d510e1a,https://www.latent.space/p/realtime-api,OpenAI Realtime API: The Missing Manual,"Everything we learned, and everything we think you need to know, from technical details on 24khz/G.711 audio, RTMP, HLS, WebRTC, to Interruption/VAD, to Cost, Latency, Tool Calls, and Context Mgmt",en,2025-10-23 12:36:16.386,2025-10-23 12:36:16.386,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b8987711-ce28-4334-8f5e-c4a5fcc7c5ff,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,realtime,realtime,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
35dc8ea9-1cb2-47ab-a0df-c737b9d59143,18,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,mathematics,mathematics,"""Advancements in Mathematical Reasoning with Large Language Models"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
e35dfd2e-f9fa-4af3-b8e3-522987f13d02,5,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,language,language,"""Advancements in Context-Aware Language Model Performance and Applications"""
d583052f-827f-409d-be97-a7c51c1c5835,17,94c217e4-d35f-442d-9d76-6a2561b15caf,https://arxiv.org/abs/2402.03300,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",en,2025-10-23 12:41:13.177,2025-10-23 12:41:13.177,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
d583052f-827f-409d-be97-a7c51c1c5835,17,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,deeplearning,deeplearning,"""Advancements in Large Language Models for Enhanced Reasoning"""
331023e7-5440-470d-9a61-756012d2d19f,4,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
331023e7-5440-470d-9a61-756012d2d19f,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,experts,experts,"""Advances in Reasoning and Learning for AI Agents"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,f6a8e377-0b53-4f09-bf43-777582c77721,https://arxiv.org/abs/2401.04088,Mixtral of Experts,"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",en,2025-10-23 12:41:40.828,2025-10-23 12:41:40.828,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
3bd21d8a-66f3-46c6-a598-353ca8fa6ef8,8,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,mixtral,mixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
f9aaf4e8-18e2-4f48-af22-4a044c4def8d,8,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,pixtral,pixtral,"""Evaluating Hallucinations in Emerging Large Language Models"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
297d274f-3286-4d1e-b228-6bfe79972c07,3,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,optimization,optimization,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,9006090a-94ba-4ee4-b92b-a57e2ff0f508,https://arxiv.org/abs/2410.07073,Pixtral 12B,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",en,2025-10-23 12:41:43.611,2025-10-23 12:41:43.611,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
95886c73-ec03-4faf-b60f-6faee2c7281e,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,performance,performance,"""Evaluating Performance Capabilities of Augmented Language Models"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,programming,programming,"""Integration of Large Language Models in Code Development"""
1e494b44-8cff-41be-aa29-25fa742b069d,1,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,programming,programming,"""Integration of Large Language Models in Code Development"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
f3a62fca-4bf1-40b4-930e-ec15c9cf376c,4,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,intelligence,intelligence,"""Advances in Reasoning and Learning for AI Agents"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,coding,coding,"""Integration of Large Language Models in Code Development"""
4ba90f6b-b138-45fc-bc59-a166c83fef53,1,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,coding,coding,"""Integration of Large Language Models in Code Development"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,00897913-7341-4b14-b64e-5cf88725e274,https://arxiv.org/abs/2401.14196,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.",en,2025-10-23 12:41:32.368,2025-10-23 12:41:32.368,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
2e075e6d-1d12-4ba0-be36-216157c4c121,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,tools,tools,"""Evaluating Performance Capabilities of Augmented Language Models"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
42de4c3b-e9b8-424d-9e2c-46670e1f3a2f,17,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,deepseek,deepseek,"""Advancements in Large Language Models for Enhanced Reasoning"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
e91a6718-0286-452a-9ffb-1947233b8c9b,11,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,llm,llm,"""Advancements in Open-Source Language Models and Benchmarking"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
07db4626-c83f-4ac5-86d4-e49ab2af51a0,3,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,scaling,scaling,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,80095e27-6b62-4ff4-84f6-5c942b64d59e,https://arxiv.org/abs/2401.02954,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",en,2025-10-23 12:41:36.609,2025-10-23 12:41:36.609,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
210c3bfc-41e9-40c3-9604-79cbbea0743e,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,longtermism,longtermism,"""Evaluating Performance Capabilities of Augmented Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,8afb7bbd-e072-436f-8811-2664fc9bedcb,https://arxiv.org/abs/2310.06825,Mistral 7B,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",en,2025-10-23 12:41:46.997,2025-10-23 12:41:46.997,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
d77e92a2-4610-40fd-a783-4e97877653d7,8,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,mistral,mistral,"""Evaluating Hallucinations in Emerging Large Language Models"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,c4fd6f2e-a950-4a27-a4e2-5b03412a2e0d,https://arxiv.org/abs/2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",en,2025-10-23 12:41:52.931,2025-10-23 12:41:52.931,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
e8109167-eded-42d9-9fa2-303cdbac39f9,11,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,llama,llama,"""Advancements in Open-Source Language Models and Benchmarking"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,42e8404b-5600-4f15-8fe4-14e81e951ab9,https://arxiv.org/abs/2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",en,2025-10-23 12:41:49.764,2025-10-23 12:41:49.764,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
c4782ac0-f61d-468b-bcb9-4277965d7b6d,14,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,chat,chat,"""Advancements in Conversational AI and Language Model Architectures"""
f7689730-8869-4ba9-967a-64db26609098,5,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
f7689730-8869-4ba9-967a-64db26609098,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,fine,fine,"""Advancements in Context-Aware Language Model Performance and Applications"""
5265e689-b19c-47aa-a508-eb9e05843065,3,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
5265e689-b19c-47aa-a508-eb9e05843065,3,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,tuning,tuning,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,gpt,gpt,"""Integration of Large Language Models in Code Development"""
a933f67a-0950-473e-9e5d-ea86eae47452,1,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,gpt,gpt,"""Integration of Large Language Models in Code Development"""
f430d1e6-eb98-4634-b822-46922c61862a,15,3792bb08-9a28-4b4e-9a74-6d1379c553b0,https://arxiv.org/abs/2303.08774,GPT-4 Technical Report,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",en,2025-10-23 12:41:56.429,2025-10-23 12:41:56.429,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,report,report,"""Evaluation and Analysis of Open Large Language Models"""
f430d1e6-eb98-4634-b822-46922c61862a,15,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,report,report,"""Evaluation and Analysis of Open Large Language Models"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,e838a036-4315-466c-a099-7519aa9460ed,https://arxiv.org/abs/2407.21783,The Llama 3 Herd of Models,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",en,2025-10-23 12:42:04.367,2025-10-23 12:42:04.367,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
d11b12e4-afdd-4b56-87af-f8a1cebd12dc,6,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,models,models,"""Advancements in Unified Architectures for Language Model Implementation"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
adb97cad-5b4d-4fe6-9a89-5b279707addd,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,segment,segment,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
fe0fcf88-552b-4479-b1a1-6838d08eb490,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,images,images,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,8c5c6882-01ec-46b3-9994-7291657240bc,https://arxiv.org/abs/2408.00714,SAM 2: Segment Anything in Images and Videos,"We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",en,2025-10-23 12:42:11.030,2025-10-23 12:42:11.030,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
f06ce96e-32f6-43c6-976c-3c01bb5ed0ac,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,videos,videos,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
1467d2cc-dfe0-4259-845f-11715a167804,5,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
1467d2cc-dfe0-4259-845f-11715a167804,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,speech,speech,"""Advancements in Context-Aware Language Model Performance and Applications"""
7afbf44d-d279-4b27-9160-93886f003daf,0,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7afbf44d-d279-4b27-9160-93886f003daf,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,recognition,recognition,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,93f76ea9-e8e8-47ed-8ea1-97f764d2b0b0,https://arxiv.org/abs/2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",en,2025-10-23 12:42:08.013,2025-10-23 12:42:08.013,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
b3ca18ab-5103-4509-900f-e7bc8bbbe1e1,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,supervision,supervision,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
49a2c518-d1e1-4125-9aed-44b8be45940c,4,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,learning,learning,"""Advances in Reasoning and Learning for AI Agents"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
3599a89f-2dd1-4973-a694-10a00f517d82,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,transferable,transferable,"""Advancements in Context-Aware Language Model Performance and Applications"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
b4b2ec77-f760-40a7-951a-8f91b687ad40,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,visual,visual,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
9e086929-b30f-4646-92da-6e203b4a4586,17,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
9e086929-b30f-4646-92da-6e203b4a4586,17,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,deep,deep,"""Advancements in Large Language Models for Enhanced Reasoning"""
ddf3887a-831b-4988-86d4-03be3b502652,5,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
ddf3887a-831b-4988-86d4-03be3b502652,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,large,large,"""Advancements in Context-Aware Language Model Performance and Applications"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,b13015ad-c128-45f5-9062-e1165a0580e7,https://arxiv.org/abs/2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",en,2025-10-23 12:42:25.948,2025-10-23 12:42:25.948,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
f4ee8616-00dd-45f3-9e63-6eb45265768d,3,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,scale,scale,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
7ac128d2-b5b6-4283-9bd6-d1204fe0a4b7,0,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,multimodal,multimodal,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
524bdf28-bed7-4b51-af8c-f739ad4c9929,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,shortcomings,shortcomings,"""Evaluating Performance Capabilities of Augmented Language Models"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,e49c6687-31a3-43bb-a228-a66c58920e62,https://arxiv.org/abs/2401.06209,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",en,2025-10-23 12:42:18.358,2025-10-23 12:42:18.358,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
d45d8730-a6f8-4636-987c-06496547bc8b,4,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,exploration,exploration,"""Advances in Reasoning and Learning for AI Agents"""
2f804abe-014e-4027-bb2c-370ba604471c,5,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
2f804abe-014e-4027-bb2c-370ba604471c,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,object,object,"""Advancements in Context-Aware Language Model Performance and Applications"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
07e2bc25-09ac-4491-bd68-b52bbbf29059,0,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,detection,detection,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,f8a6437d-1758-4456-8006-e73e052bf1f9,https://arxiv.org/abs/1506.02640,"You Only Look Once: Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",en,2025-10-23 12:42:28.654,2025-10-23 12:42:28.654,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
d67f9529-7b12-4d99-a1ee-c76bc0e1c1a4,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,unified,unified,"""Advancements in Unified Architectures for Language Model Implementation"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,code,code,"""Integration of Large Language Models in Code Development"""
4091ca3e-cad2-4e18-a88f-da7e3c367624,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,code,code,"""Integration of Large Language Models in Code Development"""
c62fa43c-9451-4056-af88-693efefc58cc,5,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
c62fa43c-9451-4056-af88-693efefc58cc,5,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,generation,generation,"""Advancements in Context-Aware Language Model Performance and Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,90acc4d3-2a28-4101-b505-d78f89df90d9,https://arxiv.org/abs/2401.08500,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium",en,2025-10-23 12:42:31.548,2025-10-23 12:42:31.548,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
4c518db6-54a9-4bb9-813d-7b030f3971f1,12,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,prompt,prompt,"""Advancements in Prompt Engineering for Language Model Applications"""
d65948db-9e54-42b3-97ab-423568e1450f,0,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
d65948db-9e54-42b3-97ab-423568e1450f,0,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,segmentation,segmentation,"""Advancements in Real-Time Visual Recognition and Segmentation Techniques"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
bf3347ab-3b15-4f7c-a852-c035c1427149,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,sonnet,sonnet,"""Advancements in Open-Source Language Models and Benchmarking"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
4cfc89fc-ee49-49f3-a939-c1ac3acc809a,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,agents,agents,"""Advances in Reasoning and Learning for AI Agents"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,eeb0873d-fc5d-495e-acee-c4c085bc8f56,https://www.latent.space/p/claude-sonnet,"The new Claude 3.5 Sonnet, Computer Use, and Building SOTA Agents ‚Äî with Erik Schluntz, Anthropic","Anthropic recently scored a huge win on OpenAI's turf by achieving SOTA on -their- SWE-Bench Verified benchmark, using an upgraded Claude 3.5 Sonnet. For the first time, they spill the beans.",en,2025-10-23 12:42:34.863,2025-10-23 12:42:34.863,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
e6bfb895-c07d-444c-b161-4cffe421b017,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,computer,computer,"""Advancements in Context-Aware Language Model Performance and Applications"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
9ab5ca7a-4876-4862-a1a8-e876099db44d,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,stack,stack,"""Advancements in Open-Source Language Models and Benchmarking"""
6884612e-0f89-483b-94fa-61996b5976f7,15,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,source,source,"""Evaluation and Analysis of Open Large Language Models"""
6884612e-0f89-483b-94fa-61996b5976f7,15,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,source,source,"""Evaluation and Analysis of Open Large Language Models"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,45ee7ec2-578b-4b6b-a3da-83f4303a3f0f,https://arxiv.org/abs/2211.15533,The Stack: 3 TB of permissively licensed source code,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",en,2025-10-23 12:42:47.276,2025-10-23 12:42:47.276,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,d3b16202-0896-485a-a069-3adbee7f3c87,https://arxiv.org/abs/2305.16291,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",en,2025-10-23 12:43:38.082,2025-10-23 12:43:38.082,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,licensing,licensing,"""Integration of Large Language Models in Code Development"""
d20ce2ae-89e0-4483-a039-cf154fac8c79,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,licensing,licensing,"""Integration of Large Language Models in Code Development"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
f49edab0-a3d5-46bf-8525-d75b280f74d0,5,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,memory,memory,"""Advancements in Context-Aware Language Model Performance and Applications"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,0ed6293c-02cb-4b5b-ae2a-3040cfd4a6da,https://x.com/swyx/status/1915128966203236571,"swyx on X: ""here's every memory paper, MECE'd. https://t.co/wexDcLkVU2"" / X",,en,2025-10-23 12:43:43.629,2025-10-23 12:43:43.629,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
ada2bf39-b93d-486e-97eb-34c730c48fdc,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,papers,papers,"""Enhancing Query Response Accuracy in Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,systems,systems,"""Advancements in Multitask Processing for Language Models"""
353b596a-ff9c-4f0d-8bba-a410cb8bb49b,7,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,systems,systems,"""Advancements in Multitask Processing for Language Models"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
f1f3fbcc-2a72-4a00-877d-46e2c0eaa770,6,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,implementation,implementation,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,96560085-1ae4-4872-8a1e-ddfbbae4f7e6,https://arxiv.org/abs/2310.08560,MemGPT: Towards LLMs as Operating Systems,"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",en,2025-10-23 12:43:47.061,2025-10-23 12:43:47.061,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
d0a85aab-e7e5-43a8-9354-17f47ab31679,6,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,architecture,architecture,"""Advancements in Unified Architectures for Language Model Implementation"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,8f08991d-97ae-412b-be65-4ad767128f9c,https://arxiv.org/abs/2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",en,2025-10-23 12:43:50.179,2025-10-23 12:43:50.179,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,cfafbecf-221c-4db3-818e-ae5c15aa49d0,https://arxiv.org/abs/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",en,2025-10-23 12:43:53.177,2025-10-23 12:43:53.177,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,d59a2731-b9c7-4d7d-802b-cb4124eb32f0,https://arxiv.org/abs/2210.03629,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",en,2025-10-23 12:44:24.507,2025-10-23 12:44:24.507,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
8a97f8e7-a2a7-469d-aaec-d5e55379ffb7,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,huggingface,huggingface,"""Advancements in Open-Source Language Models and Benchmarking"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,5dea4d29-92df-4e03-a44a-362de4e8fe1a,https://openai.com/index/swe-lancer/,Introducing the SWE-Lancer benchmark,Can frontier LLMs earn $1 million from real-world freelance software engineering?,en-US,2025-10-23 12:44:30.621,2025-10-23 12:44:30.621,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0a03aa79-1264-4599-a220-cbd062f410a1,10,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,benchmark,benchmark,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,github,github,"""Integration of Large Language Models in Code Development"""
0f3b9e01-f2fd-4792-aac6-cdc8a6e76544,1,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,github,github,"""Integration of Large Language Models in Code Development"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f2cf9c7e-8ec7-4922-bd9e-e5d54ebc62ee,9,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,issues,issues,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a890aac3-13da-41ef-a80c-0d4ef964b36c,10,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,benchmarks,benchmarks,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,eb08c79a-4663-4219-9ed3-f65363fafcdf,https://arxiv.org/abs/2310.06770,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,"Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",en,2025-10-23 12:44:33.387,2025-10-23 12:44:33.387,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,system,system,"""Advancements in Multitask Processing for Language Models"""
ba0ce7d7-5974-41f8-b58b-d851d2c04d49,7,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,system,system,"""Advancements in Multitask Processing for Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,35d05a09-2f99-4bd8-96c8-7fe64142d676,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,"Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:

In-context hallucination: The model output should be consistent with the source content in context.
Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.

This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",en,2025-10-23 12:44:36.351,2025-10-23 12:44:36.351,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
42a6f5fc-6007-44d6-b94e-492bfdc2ad70,8,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,hallucinations,hallucinations,"""Evaluating Hallucinations in Emerging Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
4d780d09-3375-4f36-ab4c-c3f37e8b796c,15,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,facts,facts,"""Evaluation and Analysis of Open Large Language Models"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
b3f0fa1b-abc2-448e-8208-3a4237a45284,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,building,building,"""Advancements in Unified Architectures for Language Model Implementation"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
d033c6ad-938b-4a9f-b21c-55ee42135150,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,retrieval,retrieval,"""Enhancing Query Response Accuracy in Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
c9aed434-1de1-4d55-8087-52646df7524f,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,augmented,augmented,"""Evaluating Performance Capabilities of Augmented Language Models"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,8ac7910d-c479-4126-95cc-9afabfa05e53,https://arxiv.org/abs/2407.07858v1,FACTS About Building Retrieval Augmented Generation-based Chatbots,"Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.""",en,2025-10-23 12:44:39.376,2025-10-23 12:44:39.376,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
1d0c874d-2ed8-47eb-98ae-a94d84f501b6,14,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,chatbots,chatbots,"""Advancements in Conversational AI and Language Model Architectures"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,85b67b67-f84c-4d98-8e4b-b5e5788dddd6,https://arxiv.org/abs/2309.15217,Ragas: Automated Evaluation of Retrieval Augmented Generation,"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",en,2025-10-23 12:45:07.907,2025-10-23 12:45:07.907,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
0f1ded92-3a77-4315-b72e-3adf525ca586,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,evaluation,evaluation,"""Evaluating Performance Capabilities of Augmented Language Models"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
bb6cbcf0-e43c-4912-a9cc-88c69a76f098,13,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,embedding,embedding,"""Advancements in Text Embeddings and Model Architectures"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
ec7d5078-2141-4c7d-8550-e73cf2864071,10,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,datasets,datasets,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
22fb7f68-3f11-4d88-8d0f-2321057a33de,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,capabilities,capabilities,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,2b603ec5-4dee-4d14-bf68-20709a0018b5,https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q,"Zach Nussbaum on X: ""üßµ Excited to announce modernbert-embed-base, a new embedding model built on the newly released ModernBERT! 

Trained on the public Nomic Embed datasets, modernbert-embed-base is a ~nomic-embed~ quality model with Matryoshka capabilities and brings the great advances of https://t.co/zBpt7RM9G4"" / X",,en,2025-10-23 12:45:23.312,2025-10-23 12:45:23.312,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
a1a2e7c6-3341-4b95-9dd0-ad7dfb0c35ee,2,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,advancements,advancements,"""Evaluating Performance Capabilities of Augmented Language Models"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,97b1b57c-098b-486c-a67e-4ccc7a5171ad,https://news.ycombinator.com/item?id=42504379,The MTEB benchmark is dead | Hacker News,,en,2025-10-23 12:45:31.947,2025-10-23 12:45:31.947,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,3abc8cac-39e6-4e7b-9b20-ba5176cbce36,https://arxiv.org/abs/2210.07316,MTEB: Massive Text Embedding Benchmark,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.",en,2025-10-23 12:45:34.914,2025-10-23 12:45:34.914,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
48c1a5c1-dd44-4aa7-8ece-ec53421d7bfa,11,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,mteb,mteb,"""Advancements in Open-Source Language Models and Benchmarking"""
764ad745-f615-45e7-bd13-22623074e6a0,4,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
764ad745-f615-45e7-bd13-22623074e6a0,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,knowledge,knowledge,"""Advances in Reasoning and Learning for AI Agents"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
c76c4d34-7115-48a2-8566-5f34362455d7,5,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,nlp,nlp,"""Advancements in Context-Aware Language Model Performance and Applications"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
9312f0fb-1974-4eae-8c22-52cb6ef6e157,7,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,tasks,tasks,"""Advancements in Multitask Processing for Language Models"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,b2e6174b-f4d7-4345-beac-a5b571f87f45,https://arxiv.org/abs/2005.11401,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",en,2025-10-23 12:45:38.829,2025-10-23 12:45:38.829,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
53dcddbb-26e1-4ad3-843b-96dc6fa99d4d,13,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,embeddings,embeddings,"""Advancements in Text Embeddings and Model Architectures"""
18a3e504-180a-45a5-af32-a208a74c3836,5,b00f3e2c-5ced-4338-9351-3495e3bdeea5,https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html,Why ‚ÄúContext Engineering‚Äù Matters,"Context engineering isn‚Äôt just another buzzword, but the emergence of a new field, community, and culture. One that will grow dramatically over the coming years. Collective realizations like these are rare in our careers. And we get to help shape this one, together.",en_US,2025-10-23 12:45:50.493,2025-10-23 12:45:50.493,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
18a3e504-180a-45a5-af32-a208a74c3836,5,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,context,context,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
d1d4e555-3026-46f4-aa25-102bed46d21f,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,power,power,"""Advancements in Context-Aware Language Model Performance and Applications"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
1f853d69-b88a-4290-a20d-9ad264f94a28,3,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,parameter,parameter,"""Enhancing Language Models Through Optimization and Scaling Techniques"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,9d0080bc-71b9-466c-b78a-dbb70b283c41,https://aclanthology.org/2021.emnlp-main.243/,The Power of Scale for Parameter-Efficient Prompt Tuning,"Brian Lester, Rami Al-Rfou, Noah Constant. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",en-us,2025-10-23 12:45:56.913,2025-10-23 12:45:56.913,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
21828f51-c9d3-4545-880e-0f23821df9fd,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,efficiency,efficiency,"""Evaluating Performance Capabilities of Augmented Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
e372de4f-f4ec-4555-bb97-4dccc4c1c825,9,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,problem,problem,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,5d9b2f6e-a61e-4e31-98e8-1860d02dd125,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",en,2025-10-23 12:45:59.717,2025-10-23 12:45:59.717,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
cb3ab243-e134-4949-bbc3-de0ebb6c3e05,9,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,solving,solving,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,644964f7-4c50-4f15-95aa-0a76f9a6ea60,https://arxiv.org/abs/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",en,2025-10-23 12:46:02.458,2025-10-23 12:46:02.458,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
689a13d2-f13e-4dda-8b89-1a69d4be002c,12,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,prompting,prompting,"""Advancements in Prompt Engineering for Language Model Applications"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,training,training,"""Advances in Reasoning and Learning for AI Agents"""
eaddf79d-5202-4f7c-96fa-50d4a5a43a3e,4,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,training,training,"""Advances in Reasoning and Learning for AI Agents"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
5addb4cc-be3f-4242-af0b-b0b8c659f2de,15,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,proof,proof,"""Evaluation and Analysis of Open Large Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
f68b2d46-5da6-4fb1-bdab-26223f06ad57,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,questions,questions,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
03b8761c-7c70-42d5-bf1a-e04aa63c36b5,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,testing,testing,"""Enhancing Query Response Accuracy in Language Models"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,c273f876-3bb8-4b0f-a7d0-c064f0972222,https://x.com/kalomaze/status/1923983739299955064,"kalomaze on X: ""here is unambiguous proof showing how @MistralAI trains on the test set!
Github NIAH test vs. custom NIAH (procedurally generated facts and questions, instead of using the exact strings from the repo) https://t.co/EhowCWkKYf"" / X",,en,2025-10-23 12:46:08.967,2025-10-23 12:46:08.967,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,64b6e864-4cec-471c-a05d-e40bcebd9dc8,https://arcprize.org/arc-agi,ARC Prize - What is ARC-AGI?,The only AI benchmark that measures AGI progress.,en_US,2025-10-23 12:46:18.490,2025-10-23 12:46:18.490,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
5caa2d19-c26f-40fb-ac36-32e79189c0a9,5,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,custom,custom,"""Advancements in Context-Aware Language Model Performance and Applications"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,20e7f9d5-cf43-49ca-a232-74bffdc69c2a,https://arxiv.org/abs/2311.07911,Instruction-Following Evaluation for Large Language Models,"One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of ""verifiable instructions"" such as ""write in more than 400 words"" and ""mention the keyword of AI at least 3 times"". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",en,2025-10-23 12:46:21.423,2025-10-23 12:46:21.423,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
f829442a-a48f-4b74-86dc-2241ddcdbe28,10,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,metrics,metrics,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,30ad7349-c883-4a59-9ee4-903a2029b060,https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",en,2025-10-23 12:46:24.007,2025-10-23 12:46:24.007,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
26b2d9aa-e9c2-44e2-8590-9cb9b8a801fb,18,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,mathematical,mathematical,"""Advancements in Mathematical Reasoning with Large Language Models"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,e48a9540-11bc-418e-9f21-e35e89f9f884,https://github.com/gkamradt/LLMTest_NeedleInAHaystack,GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy,Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack,en,2025-10-23 12:46:27.786,2025-10-23 12:46:27.786,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
a7c1a57c-2197-4e8a-8f69-490b4b886578,10,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,accuracy,accuracy,"""Evaluating Language Models through Standardized Benchmarks and Metrics"""
d641f051-654c-47b7-a908-051f584a1643,2,37726eff-de20-4d90-87f7-0e83eefa8af6,https://arxiv.org/abs/2311.12022,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",en,2025-10-23 12:46:39.383,2025-10-23 12:46:39.383,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
d641f051-654c-47b7-a908-051f584a1643,2,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,augmentation,augmentation,"""Evaluating Performance Capabilities of Augmented Language Models"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
4b4b077a-5360-495a-a0f1-9ee99b23f2a7,6,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,structure,structure,"""Advancements in Unified Architectures for Language Model Implementation"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,aa5dab6a-46ae-4bc2-b6d3-935c47ab780e,https://arxiv.org/html/2409.12640v2,Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,,en,2025-10-23 12:46:45.379,2025-10-23 12:46:45.379,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
ba56e98b-1fab-48a9-8471-19fc1504fba7,16,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,queries,queries,"""Enhancing Query Response Accuracy in Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
59d692d2-ad20-4110-bd7b-a46d0cd9ad40,7,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,multitask,multitask,"""Advancements in Multitask Processing for Language Models"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,05b2cfc3-e3b7-4358-b5f5-01f3fbdbecd1,https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",en,2025-10-23 12:46:48.825,2025-10-23 12:46:48.825,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
ebaad30e-c0f2-4401-a6fb-be52a6ca6c7a,4,d8ac7051-c5b1-45dd-842e-542d46d11cd5,https://www.answer.ai/posts/colbert-pooling.html,A little pooling goes a long way for multi-vector representations ‚Äì Answer.AI,Practical AI R&D,en,2025-10-23 12:47:45.211,2025-10-23 12:47:45.211,understanding,understanding,"""Advances in Reasoning and Learning for AI Agents"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,45cf3730-ee1c-4d17-8993-384a22ba4a1c,https://www.latent.space/p/2024-open-models,2024 in Open Models [LS Live @ NeurIPS],Luca Soldaini and Sophia Yang on how open models have exploded this past year!,en,2025-10-23 12:46:52.270,2025-10-23 12:46:52.270,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,e7ac785b-0a4a-4b11-b8b6-469d5dd4357b,https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/,"[AINews] Olympus has dropped (aka, Amazon Nova Micro|Lite|Pro|Premier|Canvas|Reel)","Amazon Bedrock is all you need? AI News for 12/2/2024-12/3/2024. We checked 7 subreddits, 433 Twitters and 29 Discords (198 channels, and 2914 messages) for...",en,2025-10-23 12:46:55.691,2025-10-23 12:46:55.691,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
f361ee75-db2e-44ff-aa49-f09cbd8e44da,9,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,problems,problems,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,c23de130-b490-48d1-b3d8-71e66d24744f,https://arxiv.org/abs/2210.09261,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",en,2025-10-23 12:46:42.293,2025-10-23 12:46:42.293,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,a912a236-51df-4807-a16c-b8268a47418f,https://arxiv.org/abs/2412.08905,Phi-4 Technical Report,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",en,2025-10-23 12:46:58.676,2025-10-23 12:46:58.676,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,cce9c262-7902-4be3-9a7c-102037d972b5,https://buttondown.com/ainews/archive/ainews-to-be-named-2748/,"[AINews] Nemotron-4-340B: NVIDIA's new large open models, built on syndata, great for syndata","Synthetic Data is 98% of all you need. AI News for 6/13/2024-6/14/2024. We checked 7 subreddits, 384 Twitters and 30 Discords (414 channels, and 2481...",en,2025-10-23 12:47:02.328,2025-10-23 12:47:02.328,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,e5852544-9fb7-4d34-9768-736387a816db,https://cohere.com/command,Cohere Command Models: AI-Powered Solutions for Enterprise,Cohere Command is a family of highly scalable language models that balances high performance with strong accuracy.,en-US,2025-10-23 12:47:05.453,2025-10-23 12:47:05.453,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,60472740-769d-4190-a04d-5e3ec64ccf82,https://arxiv.org/abs/2407.21075,Apple Intelligence Foundation Language Models,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",en,2025-10-23 12:47:08.473,2025-10-23 12:47:08.473,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,0b741823-9de2-4d08-b03a-76c6306aa8ba,https://arxiv.org/abs/2409.17146,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",en,2025-10-23 12:47:11.455,2025-10-23 12:47:11.455,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,c6c0d547-a5de-4d68-9a48-c2f0e84755c4,https://x.com/rasbt/status/1965798055141429523?s=46,"Sebastian Raschka on X: ""Updated &amp; turned my Big LLM Architecture Comparison article into a narrated video lecture. 

The 11 LLM architectures covered in this video:
1. DeepSeek V3/R1
2. OLMo 2
3. Gemma 3 
4. Mistral Small 3.1
5. Llama 4
6. Qwen3
7. SmolLM3
8. Kimi 2
9. GPT-OSS
10. Grok 2.5
11. GLM-4.5 https://t.co/POjLPWainM"" / X",,en,2025-10-23 12:47:50.008,2025-10-23 12:47:50.008,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
5fe40731-7bf9-4b73-9e15-bb26b07ac559,9,d8ae06ab-0b62-4115-86c8-7153834e142a,https://arxiv.org/abs/2502.19587,NeoBERT: A Next-Generation BERT,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.",en,2025-10-23 12:47:54.080,2025-10-23 12:47:54.080,solutions,solutions,"""Evaluating Problem-Solving Capabilities of Large Language Models"""
