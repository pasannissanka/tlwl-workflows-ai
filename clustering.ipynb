{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a591737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1\n",
      "['longtermism', 'report', 'shortcomings']\n",
      "\n",
      "Cluster  2\n",
      "['reasoning', 'llama', 'fine', 'large', 'code', 'prompt', 'stack', 'memory', 'issues', 'hallucinations', 'facts', 'context', 'power', 'parameter', 'problem', 'solving', 'prompting', 'proof', 'questions', 'testing', 'custom', 'queries', 'understanding', 'problems', 'solutions']\n",
      "\n",
      "Cluster  3\n",
      "['model', 'realtime', 'optimization', 'models', 'segment', 'images', 'videos', 'recognition', 'visual', 'object', 'detection', 'segmentation', 'systems', 'implementation', 'benchmark', 'benchmarks', 'system', 'retrieval', 'evaluation', 'embedding', 'datasets', 'embeddings', 'metrics', 'accuracy', 'structure']\n",
      "\n",
      "Cluster  4\n",
      "['engineering', 'research', 'mathematics', 'performance', 'tools', 'scaling', 'scale', 'unified', 'generation', 'papers', 'architecture', 'building', 'augmented', 'capabilities', 'advancements', 'efficiency', 'mathematical', 'augmentation']\n",
      "\n",
      "Cluster  5\n",
      "['ai', 'reading', 'reinforcement', 'curation', 'chatgpt', 'api', 'language', 'deeplearning', 'experts', 'mixtral', 'pixtral', 'programming', 'intelligence', 'coding', 'deepseek', 'llm', 'mistral', 'chat', 'tuning', 'gpt', 'speech', 'supervision', 'learning', 'transferable', 'deep', 'multimodal', 'exploration', 'sonnet', 'agents', 'computer', 'source', 'licensing', 'huggingface', 'github', 'chatbots', 'mteb', 'knowledge', 'nlp', 'tasks', 'training', 'multitask']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = [\n",
    "  \"ai\",\n",
    "  \"engineering\",\n",
    "  \"reading\",\n",
    "  \"research\",\n",
    "  \"reasoning\",\n",
    "  \"model\",\n",
    "  \"reinforcement\",\n",
    "  \"curation\",\n",
    "  \"chatgpt\",\n",
    "  \"api\",\n",
    "  \"realtime\",\n",
    "  \"mathematics\",\n",
    "  \"language\",\n",
    "  \"deeplearning\",\n",
    "  \"experts\",\n",
    "  \"mixtral\",\n",
    "  \"pixtral\",\n",
    "  \"optimization\",\n",
    "  \"performance\",\n",
    "  \"programming\",\n",
    "  \"intelligence\",\n",
    "  \"coding\",\n",
    "  \"tools\",\n",
    "  \"deepseek\",\n",
    "  \"llm\",\n",
    "  \"scaling\",\n",
    "  \"longtermism\",\n",
    "  \"mistral\",\n",
    "  \"llama\",\n",
    "  \"chat\",\n",
    "  \"fine\",\n",
    "  \"tuning\",\n",
    "  \"gpt\",\n",
    "  \"report\",\n",
    "  \"models\",\n",
    "  \"segment\",\n",
    "  \"images\",\n",
    "  \"videos\",\n",
    "  \"speech\",\n",
    "  \"recognition\",\n",
    "  \"supervision\",\n",
    "  \"learning\",\n",
    "  \"transferable\",\n",
    "  \"visual\",\n",
    "  \"deep\",\n",
    "  \"large\",\n",
    "  \"scale\",\n",
    "  \"multimodal\",\n",
    "  \"shortcomings\",\n",
    "  \"exploration\",\n",
    "  \"object\",\n",
    "  \"detection\",\n",
    "  \"unified\",\n",
    "  \"code\",\n",
    "  \"generation\",\n",
    "  \"prompt\",\n",
    "  \"segmentation\",\n",
    "  \"sonnet\",\n",
    "  \"agents\",\n",
    "  \"computer\",\n",
    "  \"stack\",\n",
    "  \"source\",\n",
    "  \"licensing\",\n",
    "  \"memory\",\n",
    "  \"papers\",\n",
    "  \"systems\",\n",
    "  \"implementation\",\n",
    "  \"architecture\",\n",
    "  \"huggingface\",\n",
    "  \"benchmark\",\n",
    "  \"github\",\n",
    "  \"issues\",\n",
    "  \"benchmarks\",\n",
    "  \"system\",\n",
    "  \"hallucinations\",\n",
    "  \"facts\",\n",
    "  \"building\",\n",
    "  \"retrieval\",\n",
    "  \"augmented\",\n",
    "  \"chatbots\",\n",
    "  \"evaluation\",\n",
    "  \"embedding\",\n",
    "  \"datasets\",\n",
    "  \"capabilities\",\n",
    "  \"advancements\",\n",
    "  \"mteb\",\n",
    "  \"knowledge\",\n",
    "  \"nlp\",\n",
    "  \"tasks\",\n",
    "  \"embeddings\",\n",
    "  \"context\",\n",
    "  \"power\",\n",
    "  \"parameter\",\n",
    "  \"efficiency\",\n",
    "  \"problem\",\n",
    "  \"solving\",\n",
    "  \"prompting\",\n",
    "  \"training\",\n",
    "  \"proof\",\n",
    "  \"questions\",\n",
    "  \"testing\",\n",
    "  \"custom\",\n",
    "  \"metrics\",\n",
    "  \"mathematical\",\n",
    "  \"accuracy\",\n",
    "  \"augmentation\",\n",
    "  \"structure\",\n",
    "  \"queries\",\n",
    "  \"multitask\",\n",
    "  \"understanding\",\n",
    "  \"problems\",\n",
    "  \"solutions\"\n",
    "]\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "\n",
    "# Perform kmean clustering\n",
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i + 1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab848afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  2 16 16  4  6  4  5 14  1  0 18  5 17  4  8  8  3  2  1  4  1  2 17\n",
      " 11  3  2  8 11 14  5  3  1 15  6  0  0  0  5  0  4  4  5  0 17  5  3  0\n",
      "  2  4  5  0  6  1  5 12  0 11  4  5 11 15  1  5 16  7  6  6 11 10  1  9\n",
      " 10  7  8 15  6 16  2 14  2 13 10  2  2 11  4  5  7 13  5  5  3  2  9  9\n",
      " 12  4 15 16 16  5 10 18 10  2  6 16  7  4  9  9]\n",
      "{np.int64(4): ['ai', 'reasoning', 'reinforcement', 'experts', 'intelligence', 'supervision', 'learning', 'exploration', 'agents', 'knowledge', 'training', 'understanding'], np.int64(2): ['engineering', 'performance', 'tools', 'longtermism', 'shortcomings', 'augmented', 'evaluation', 'capabilities', 'advancements', 'efficiency', 'augmentation'], np.int64(16): ['reading', 'research', 'papers', 'retrieval', 'questions', 'testing', 'queries'], np.int64(6): ['model', 'models', 'unified', 'implementation', 'architecture', 'building', 'structure'], np.int64(5): ['curation', 'language', 'fine', 'speech', 'transferable', 'large', 'object', 'generation', 'computer', 'memory', 'nlp', 'context', 'power', 'custom'], np.int64(14): ['chatgpt', 'chat', 'chatbots'], np.int64(1): ['api', 'programming', 'coding', 'gpt', 'code', 'licensing', 'github'], np.int64(0): ['realtime', 'segment', 'images', 'videos', 'recognition', 'visual', 'multimodal', 'detection', 'segmentation'], np.int64(18): ['mathematics', 'mathematical'], np.int64(17): ['deeplearning', 'deepseek', 'deep'], np.int64(8): ['mixtral', 'pixtral', 'mistral', 'hallucinations'], np.int64(3): ['optimization', 'scaling', 'tuning', 'scale', 'parameter'], np.int64(11): ['llm', 'llama', 'sonnet', 'stack', 'huggingface', 'mteb'], np.int64(15): ['report', 'source', 'facts', 'proof'], np.int64(12): ['prompt', 'prompting'], np.int64(7): ['systems', 'system', 'tasks', 'multitask'], np.int64(10): ['benchmark', 'benchmarks', 'datasets', 'metrics', 'accuracy'], np.int64(9): ['issues', 'problem', 'solving', 'problems', 'solutions'], np.int64(13): ['embedding', 'embeddings']}\n",
      "Cluster  5\n",
      "['ai', 'reasoning', 'reinforcement', 'experts', 'intelligence', 'supervision', 'learning', 'exploration', 'agents', 'knowledge', 'training', 'understanding']\n",
      "\n",
      "Cluster  3\n",
      "['engineering', 'performance', 'tools', 'longtermism', 'shortcomings', 'augmented', 'evaluation', 'capabilities', 'advancements', 'efficiency', 'augmentation']\n",
      "\n",
      "Cluster  17\n",
      "['reading', 'research', 'papers', 'retrieval', 'questions', 'testing', 'queries']\n",
      "\n",
      "Cluster  7\n",
      "['model', 'models', 'unified', 'implementation', 'architecture', 'building', 'structure']\n",
      "\n",
      "Cluster  6\n",
      "['curation', 'language', 'fine', 'speech', 'transferable', 'large', 'object', 'generation', 'computer', 'memory', 'nlp', 'context', 'power', 'custom']\n",
      "\n",
      "Cluster  15\n",
      "['chatgpt', 'chat', 'chatbots']\n",
      "\n",
      "Cluster  2\n",
      "['api', 'programming', 'coding', 'gpt', 'code', 'licensing', 'github']\n",
      "\n",
      "Cluster  1\n",
      "['realtime', 'segment', 'images', 'videos', 'recognition', 'visual', 'multimodal', 'detection', 'segmentation']\n",
      "\n",
      "Cluster  19\n",
      "['mathematics', 'mathematical']\n",
      "\n",
      "Cluster  18\n",
      "['deeplearning', 'deepseek', 'deep']\n",
      "\n",
      "Cluster  9\n",
      "['mixtral', 'pixtral', 'mistral', 'hallucinations']\n",
      "\n",
      "Cluster  4\n",
      "['optimization', 'scaling', 'tuning', 'scale', 'parameter']\n",
      "\n",
      "Cluster  12\n",
      "['llm', 'llama', 'sonnet', 'stack', 'huggingface', 'mteb']\n",
      "\n",
      "Cluster  16\n",
      "['report', 'source', 'facts', 'proof']\n",
      "\n",
      "Cluster  13\n",
      "['prompt', 'prompting']\n",
      "\n",
      "Cluster  8\n",
      "['systems', 'system', 'tasks', 'multitask']\n",
      "\n",
      "Cluster  11\n",
      "['benchmark', 'benchmarks', 'datasets', 'metrics', 'accuracy']\n",
      "\n",
      "Cluster  10\n",
      "['issues', 'problem', 'solving', 'problems', 'solutions']\n",
      "\n",
      "Cluster  14\n",
      "['embedding', 'embeddings']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Some models don't automatically normalize the embeddings, in which case you should normalize the embeddings:\n",
    "corpus_embeddings_2 = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Perform agglomerative clustering\n",
    "clustering_model_2 = AgglomerativeClustering(\n",
    "    n_clusters=None, distance_threshold=1.5\n",
    ")  # , affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "clustering_model_2.fit(corpus_embeddings_2)\n",
    "cluster_assignment_2 = clustering_model_2.labels_\n",
    "\n",
    "print(cluster_assignment_2)\n",
    "\n",
    "clustered_sentences_2 = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment_2):\n",
    "    if cluster_id not in clustered_sentences_2:\n",
    "        clustered_sentences_2[cluster_id] = []\n",
    "\n",
    "    clustered_sentences_2[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "for i, cluster in clustered_sentences_2.items():\n",
    "    print(\"Cluster \", i + 1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
